# ğŸ§  Explainable TrOCR  
### Making Transformer-based OCR Interpretable using Cross-Attention Visualization

---

## ğŸ” Overview

Optical Character Recognition (OCR) systems are widely used in document processing, finance, education, and legal workflows.  
While modern OCR models achieve high accuracy, they often behave as **black boxes**, providing no explanation for *why* a particular word was predicted.

This project focuses on **Explainable OCR**, built on top of **Microsoftâ€™s TrOCR (Transformer-based OCR)** model.

Instead of only extracting text, this project visualizes **where the model attends in the image for each predicted token**, using **cross-attention maps** from the Transformer decoder.

ğŸ¯ **Goal**  
Understand model behavior at inference time â€” **without retraining the model**

---

## âœ¨ Key Features

- âœ… Pretrained **TrOCR inference** (no retraining required)
- ğŸ” **Cross-attention extraction** from Transformer decoder
- ğŸ§  Token-level explainability
- ğŸ”¥ Attention **heatmap overlay** on input image
- ğŸ“Š Patch-level visual grounding of text predictions
- ğŸš€ Runs on **Google Colab (Tesla T4 GPU)** or local machine

---

